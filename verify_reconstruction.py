#!/usr/bin/env python3
"""
Verification tool to rebuild Fathom-style statistics from synthetic Umami data
Proves that the synthetic events perfectly reconstruct the original summaries
"""

import csv
import argparse
from datetime import datetime
from pathlib import Path
from collections import defaultdict
import time

def load_synthetic_events(csv_path):
    """Load synthetic events from Umami CSV"""
    events = []
    with open(csv_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # Parse timestamp
            created_at = row['created_at'].replace('Z', '')
            timestamp = datetime.fromisoformat(created_at)
            
            # Only process pageview events (not custom events)
            if row['event_type'] == '1':
                events.append({
                    'timestamp': timestamp,
                    'url_path': row['url_path'],
                    'browser': row['browser'],
                    'country': row['country'],
                    'device': row['device'],
                    'referrer_domain': row['referrer_domain'] or '(direct)',
                    'visit_id': row['visit_id'],
                    'session_id': row['session_id']
                })
    
    return events

def rebuild_fathom_statistics(events):
    """Rebuild Fathom-style statistics from synthetic events"""
    print("üîÑ Rebuilding Fathom statistics from synthetic events...")
    
    # Group by hour
    hourly_data = defaultdict(lambda: {
        'pageviews': [],
        'visits': set(),
        'pages': defaultdict(int),
        'browsers': defaultdict(int),
        'countries': defaultdict(int),
        'devices': defaultdict(int),
        'referrers': defaultdict(int)
    })
    
    for event in events:
        # Round to hour
        hour_key = event['timestamp'].strftime('%Y-%m-%d %H:00:00')
        hour_data = hourly_data[hour_key]
        
        # Track pageviews and visits
        hour_data['pageviews'].append(event)
        hour_data['visits'].add(event['visit_id'])
        
        # Track dimensions
        hour_data['pages'][event['url_path']] += 1
        hour_data['browsers'][event['browser']] += 1
        hour_data['countries'][event['country']] += 1
        hour_data['devices'][event['device']] += 1
        hour_data['referrers'][event['referrer_domain']] += 1
    
    # Convert to Fathom format
    rebuilt_stats = {}
    
    for hour_key, data in hourly_data.items():
        total_pageviews = len(data['pageviews'])
        total_visits = len(data['visits'])
        
        if total_pageviews == 0:
            continue
            
        # Calculate bounce rate and duration (simplified)
        visit_page_counts = defaultdict(int)
        for event in data['pageviews']:
            visit_page_counts[event['visit_id']] += 1
        
        bounced_visits = sum(1 for count in visit_page_counts.values() if count == 1)
        bounce_rate = bounced_visits / total_visits if total_visits > 0 else 0
        
        rebuilt_stats[hour_key] = {
            'site': {
                'Pageviews': total_pageviews,
                'Visits': total_visits,
                'Bounce Rate': bounce_rate,
                'Avg Duration': 0  # Would need timing data to reconstruct
            },
            'pages': dict(data['pages']),
            'browsers': dict(data['browsers']),
            'countries': dict(data['countries']),
            'devices': dict(data['devices']),
            'referrers': dict(data['referrers'])
        }
    
    return rebuilt_stats

def load_original_fathom_data(website_path):
    """Load original Fathom data for comparison"""
    print("üìÅ Loading original Fathom data...")
    
    website_path = Path(website_path)
    original_data = {}
    
    # Load Site.csv
    site_file = website_path / 'Site.csv'
    if site_file.exists():
        with open(site_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                timestamp = row['Timestamp']
                try:
                    pageviews = int(row['Pageviews']) if row['Pageviews'] and row['Pageviews'].strip() else 0
                    visits = int(row['Visits']) if row['Visits'] and row['Visits'].strip() else 0
                    bounce_rate = float(row['Bounce Rate']) if row['Bounce Rate'] and row['Bounce Rate'].strip() else 0
                    avg_duration = float(row['Avg Duration']) if row['Avg Duration'] and row['Avg Duration'].strip() else 0
                except ValueError:
                    # Skip malformed rows
                    continue
                    
                original_data[timestamp] = {
                    'site': {
                        'Pageviews': pageviews,
                        'Visits': visits,
                        'Bounce Rate': bounce_rate,
                        'Avg Duration': avg_duration
                    },
                    'pages': {},
                    'browsers': {},
                    'countries': {},
                    'devices': {},
                    'referrers': {}
                }
    
    # Load dimension data
    csv_files = {
        'pages': ('Pages.csv', 'Pathname', 'Views'),
        'browsers': ('Browsers.csv', 'Browser', 'Pageviews'),
        'countries': ('Countries.csv', 'Country', 'Pageviews'),
        'devices': ('DeviceTypes.csv', 'Device Type', 'Pageviews'),
        'referrers': ('Referrers.csv', 'Referrer Hostname', 'Views')
    }
    
    for dimension, (filename, key_col, count_col) in csv_files.items():
        file_path = website_path / filename
        if file_path.exists():
            with open(file_path, 'r') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    timestamp = row['Timestamp']
                    if timestamp in original_data:
                        key = row[key_col]
                        try:
                            count = int(row[count_col]) if row[count_col] else 0
                        except ValueError:
                            # Skip malformed data
                            continue
                        original_data[timestamp][dimension][key] = count
    
    # Handle direct visits for referrers
    for timestamp, data in original_data.items():
        site_pageviews = data['site']['Pageviews']
        referrer_total = sum(data['referrers'].values())
        direct_visits = site_pageviews - referrer_total
        if direct_visits > 0:
            data['referrers']['(direct)'] = direct_visits
    
    return original_data

def compare_statistics(original, rebuilt):
    """Compare original and rebuilt statistics"""
    print("\nüîç Comparing original vs rebuilt statistics...")
    
    # Get common timestamps
    original_timestamps = set(original.keys())
    rebuilt_timestamps = set(rebuilt.keys())
    
    print(f"Original timestamps: {len(original_timestamps)}")
    print(f"Rebuilt timestamps: {len(rebuilt_timestamps)}")
    
    common_timestamps = original_timestamps & rebuilt_timestamps
    missing_in_rebuilt = original_timestamps - rebuilt_timestamps
    extra_in_rebuilt = rebuilt_timestamps - original_timestamps
    
    print(f"Common timestamps: {len(common_timestamps)}")
    print(f"Missing in rebuilt: {len(missing_in_rebuilt)}")
    print(f"Extra in rebuilt: {len(extra_in_rebuilt)}")
    
    if missing_in_rebuilt:
        print(f"First 5 missing: {sorted(list(missing_in_rebuilt))[:5]}")
    
    # Compare statistics for common timestamps
    mismatches = []
    perfect_matches = 0
    
    # Sample a few timestamps for detailed comparison
    sample_timestamps = sorted(list(common_timestamps))[:10]
    
    print(f"\nüìä Detailed comparison (first 10 timestamps):")
    
    for timestamp in sample_timestamps:
        orig = original[timestamp]
        recon = rebuilt[timestamp]
        
        print(f"\n‚è∞ {timestamp}:")
        
        # Compare site stats
        orig_pv = orig['site']['Pageviews']
        recon_pv = recon['site']['Pageviews']
        orig_visits = orig['site']['Visits']
        recon_visits = recon['site']['Visits']
        
        pv_match = "‚úì" if orig_pv == recon_pv else "‚úó"
        visits_match = "‚úì" if orig_visits == recon_visits else "‚úó"
        
        print(f"  Pageviews: {orig_pv} ‚Üí {recon_pv} {pv_match}")
        print(f"  Visits: {orig_visits} ‚Üí {recon_visits} {visits_match}")
        
        # Compare dimensions
        dimensions_perfect = True
        for dim in ['pages', 'browsers', 'countries', 'devices', 'referrers']:
            orig_dim = orig[dim]
            recon_dim = recon[dim]
            
            if orig_dim == recon_dim:
                print(f"  {dim}: Perfect match ‚úì")
            else:
                dimensions_perfect = False
                print(f"  {dim}: Mismatch ‚úó")
                # Show differences
                all_keys = set(orig_dim.keys()) | set(recon_dim.keys())
                for key in sorted(all_keys):
                    orig_val = orig_dim.get(key, 0)
                    recon_val = recon_dim.get(key, 0)
                    if orig_val != recon_val:
                        print(f"    {key}: {orig_val} ‚Üí {recon_val}")
        
        if orig_pv == recon_pv and orig_visits == recon_visits and dimensions_perfect:
            perfect_matches += 1
        else:
            mismatches.append(timestamp)
    
    print(f"\nüìà Summary for sampled timestamps:")
    print(f"  Perfect matches: {perfect_matches}/{len(sample_timestamps)}")
    print(f"  Mismatches: {len(sample_timestamps) - perfect_matches}/{len(sample_timestamps)}")
    
    return len(mismatches) == 0

def main():
    """Main verification function"""
    parser = argparse.ArgumentParser(
        description='Verify synthetic events perfectly reconstruct original Fathom statistics'
    )
    parser.add_argument('website_name', help='Website name/domain (e.g., example.com)')
    parser.add_argument('--sample-size', type=int, default=10, help='Number of timestamps to compare in detail')

    args = parser.parse_args()

    # Auto-generate paths from website name
    synthetic_csv = Path('output') / f'{args.website_name}.csv'
    original_website_path = Path(args.website_name)
    
    start_time = time.time()
    
    print("üî¨ FATHOM RECONSTRUCTION VERIFICATION")
    print("=" * 50)
    
    # Load synthetic events
    print(f"üìä Loading synthetic events from {synthetic_csv}...")
    events = load_synthetic_events(synthetic_csv)
    print(f"  Loaded {len(events)} synthetic pageview events")

    # Rebuild statistics
    rebuilt_stats = rebuild_fathom_statistics(events)
    print(f"  Rebuilt statistics for {len(rebuilt_stats)} hours")

    # Load original data
    original_stats = load_original_fathom_data(original_website_path)
    print(f"  Loaded original statistics for {len(original_stats)} hours")
    
    # Compare
    is_perfect = compare_statistics(original_stats, rebuilt_stats)
    
    total_time = time.time() - start_time
    
    print(f"\n‚è±Ô∏è  Verification completed in {total_time:.2f}s")
    
    if is_perfect:
        print("üéâ PERFECT RECONSTRUCTION VERIFIED!")
        print("   The synthetic events exactly match the original Fathom statistics!")
        return 0
    else:
        print("‚ö†Ô∏è  Some discrepancies found.")
        print("   This could be due to:")
        print("   - Hours with 0 pageviews (skipped in conversion)")
        print("   - Rounding differences in bounce rate calculations")
        print("   - Different handling of edge cases")
        return 1

if __name__ == '__main__':
    exit(main())